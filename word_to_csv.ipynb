{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, load one Word document for testing\n",
    "doc = Document(\"your_file.docx\")  # Replace with the actual file name\n",
    "doc.paragraphs\n",
    "\n",
    "# Extract all non-empty paragraphs\n",
    "paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "\n",
    "# Print basic information about the document\n",
    "print(f\"Document has {len(paragraphs)} paragraphs\")\n",
    "# Save the first 50 paragraphs to a text file\n",
    "with open(\"first_50_paragraphs.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(paragraphs[:50]))\n",
    "print(\"First 50 paragraphs saved to 'first_50_paragraphs.txt'\")\n",
    "\n",
    "# Analyze content of articles\n",
    "# Ensure articles and df are defined\n",
    "if 'articles' in globals() and 'df' in globals():\n",
    "    articles_df = pd.DataFrame(articles)\n",
    "    if len(articles) > 0:\n",
    "        # Number of articles\n",
    "        print(f\"\\nTotal articles found: {len(articles)}\")\n",
    "        \n",
    "        # Years distribution (if 'year' column exists)\n",
    "        if 'year' in df.columns:\n",
    "            years = df['year'].value_counts().sort_index()\n",
    "            print(f\"\\nArticles by year:\\n{years}\")\n",
    "        else:\n",
    "            print(\"\\nNo 'year' column found in the DataFrame.\")\n",
    "        \n",
    "        # Common topics\n",
    "        from collections import Counter\n",
    "        import re\n",
    "        \n",
    "        # Get all words from headlines\n",
    "        all_words = ' '.join(df['headline'].str.lower()).split()\n",
    "        # Remove common stop words\n",
    "        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', 'to', 'for', 'in', 'on', 'with', 'of', 'by', 'as'}\n",
    "        filtered_words = [word for word in all_words if word not in stop_words and len(word) > 2]\n",
    "        \n",
    "        # Count and display common words\n",
    "        word_counts = Counter(filtered_words)\n",
    "        most_common = word_counts.most_common(15)\n",
    "        print(\"\\nMost common words in headlines:\")\n",
    "        for word, count in most_common:\n",
    "            print(f\"  {word}: {count}\")\n",
    "        \n",
    "        # Length statistics (if 'body_length' and 'headline_length' columns exist)\n",
    "        if 'body_length' in df.columns and 'headline_length' in df.columns:\n",
    "            print(f\"\\nAverage body length: {df['body_length'].mean():.1f} characters\")\n",
    "            print(f\"Average headline length: {df['headline_length'].mean():.1f} characters\")\n",
    "        else:\n",
    "            print(\"\\nLength statistics columns not found in the DataFrame.\")\n",
    "        \n",
    "        # Article sources (if available)\n",
    "        if 'source' in df.columns and df['source'].notna().any():\n",
    "            sources = df['source'].value_counts().head(5)\n",
    "            print(f\"\\nTop sources:\\n{sources}\")\n",
    "else:\n",
    "    print(\"Variables 'articles' and 'df' are not defined in the current scope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Word document\n",
    "doc = Document(\"your_file.docx\")  # Replace with your actual file path\n",
    "\n",
    "# Extract all non-empty paragraphs\n",
    "paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "\n",
    "# Split articles using \"End of Document\" as a delimiter\n",
    "def split_articles_by_marker(paragraphs):\n",
    "    articles = []\n",
    "    current_article = []\n",
    "    for para in paragraphs:\n",
    "        current_article.append(para)\n",
    "        if \"End of Document\" in para:\n",
    "            articles.append(current_article)\n",
    "            current_article = []\n",
    "    if current_article:\n",
    "        articles.append(current_article)\n",
    "    return articles\n",
    "\n",
    "# Revised function to extract fields from a single article block\n",
    "def extract_article_fields(article):\n",
    "    fields = {\n",
    "        \"headline\": \"\",\n",
    "        \"source\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"copyright\": \"\",\n",
    "        \"section\": \"\",\n",
    "        \"length\": \"\",\n",
    "        \"byline\": \"\",\n",
    "        \"body\": \"\",\n",
    "        \"notes\": \"\",\n",
    "        \"load_date\": \"\"\n",
    "    }\n",
    "    \n",
    "    body_lines = []\n",
    "    note_lines = []\n",
    "    \n",
    "    # Flags for collecting body/notes\n",
    "    in_body = False\n",
    "    in_notes = False\n",
    "    \n",
    "    # Collect header lines (before any marker)\n",
    "    header_lines = []\n",
    "    \n",
    "    for line in article:\n",
    "        # Check if line is a marker\n",
    "        if line == \"Body\":\n",
    "            in_body = True\n",
    "            in_notes = False\n",
    "            continue\n",
    "        elif line == \"Notes\":\n",
    "            in_body = False\n",
    "            in_notes = True\n",
    "            continue\n",
    "        elif \"End of Document\" in line:\n",
    "            in_body = False\n",
    "            in_notes = False\n",
    "            continue\n",
    "        elif line.startswith(\"Load-Date:\"):\n",
    "            fields[\"load_date\"] = line.replace(\"Load-Date:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Section:\"):\n",
    "            fields[\"section\"] = line.replace(\"Section:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Length:\"):\n",
    "            fields[\"length\"] = line.replace(\"Length:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Byline:\"):\n",
    "            fields[\"byline\"] = line.replace(\"Byline:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Copyright\"):\n",
    "            fields[\"copyright\"] += line + \" \"\n",
    "            continue\n",
    "        \n",
    "        # Depending on our state, add line to header, body, or notes\n",
    "        if in_body:\n",
    "            body_lines.append(line)\n",
    "        elif in_notes:\n",
    "            note_lines.append(line)\n",
    "        else:\n",
    "            header_lines.append(line)\n",
    "    \n",
    "    # Process header_lines:\n",
    "    # Assume the first line is the headline\n",
    "    if header_lines:\n",
    "        fields[\"headline\"] = header_lines[0]\n",
    "        \n",
    "        # Next, concatenate subsequent lines (until we hit a date-like pattern) as source.\n",
    "        source_lines = []\n",
    "        date_line = \"\"\n",
    "        for line in header_lines[1:]:\n",
    "            # Check for a date pattern (e.g., \"November 18, 2024\")\n",
    "            if re.search(r\"[A-Za-z]+\\s+\\d{1,2},\\s*\\d{4}\", line):\n",
    "                date_line = line\n",
    "                break\n",
    "            else:\n",
    "                source_lines.append(line)\n",
    "        fields[\"source\"] = \" \".join(source_lines)\n",
    "        fields[\"date\"] = date_line\n",
    "\n",
    "    fields[\"body\"] = \"\\n\".join(body_lines)\n",
    "    fields[\"notes\"] = \"\\n\".join(note_lines)\n",
    "    \n",
    "    return fields\n",
    "\n",
    "# Process the document and extract articles\n",
    "articles = split_articles_by_marker(paragraphs)\n",
    "structured_data = [extract_article_fields(article) for article in articles]\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df = pd.DataFrame(structured_data)\n",
    "csv_path = \"output_articles_revised.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Gmail API scope to read messages\n",
    "SCOPES = ['https://www.googleapis.com/auth/gmail.readonly']\n",
    "\n",
    "# Folder where attachments will be saved\n",
    "ATTACH_DIR = 'gmail_attachments'\n",
    "os.makedirs(ATTACH_DIR, exist_ok=True)\n",
    "\n",
    "def authenticate():\n",
    "    \"\"\"Authenticate and return the Gmail API service.\"\"\"\n",
    "    creds = None\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    return build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "def save_attachment(service, msg_id):\n",
    "    \"\"\"Download and save attachments from a message, with unique filenames.\"\"\"\n",
    "    try:\n",
    "        msg = service.users().messages().get(userId='me', id=msg_id).execute()\n",
    "        payload = msg.get('payload', {})\n",
    "        parts = payload.get('parts', [])\n",
    "        attachment_count = 0\n",
    "\n",
    "        for part in parts:\n",
    "            filename = part.get('filename')\n",
    "            body = part.get('body', {})\n",
    "            if filename and 'attachmentId' in body:\n",
    "                att_id = body['attachmentId']\n",
    "                attachment = service.users().messages().attachments().get(\n",
    "                    userId='me', messageId=msg_id, id=att_id).execute()\n",
    "                data = attachment.get('data')\n",
    "                file_data = base64.urlsafe_b64decode(data.encode('UTF-8'))\n",
    "\n",
    "                # Ensure unique filename\n",
    "                name, ext = os.path.splitext(filename)\n",
    "                safe_msg_id = msg_id.replace('/', '_')\n",
    "                unique_name = f\"{name}__{safe_msg_id}__{attachment_count}{ext}\"\n",
    "                file_path = os.path.join(ATTACH_DIR, unique_name)\n",
    "\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(file_data)\n",
    "                print(f\"Downloaded: {unique_name}\")\n",
    "                attachment_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with message {msg_id}: {e}\")\n",
    "\n",
    "def download_all_attachments(service):\n",
    "    \"\"\"Loop through Gmail messages in Primary tab and download their attachments.\"\"\"\n",
    "    user_id = 'me'\n",
    "    page_token = None\n",
    "    total_processed = 0\n",
    "\n",
    "    while True:\n",
    "        response = service.users().messages().list(\n",
    "            userId=user_id,\n",
    "            q='has:attachment category:primary',\n",
    "            maxResults=100,\n",
    "            pageToken=page_token\n",
    "        ).execute()\n",
    "\n",
    "        messages = response.get('messages', [])\n",
    "        print(f\"Processing {len(messages)} messages...\")\n",
    "\n",
    "        for msg in messages:\n",
    "            save_attachment(service, msg['id'])\n",
    "            total_processed += 1\n",
    "\n",
    "        page_token = response.get('nextPageToken')\n",
    "        if not page_token:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nDone. Processed {total_processed} messages.\")\n",
    "\n",
    "# Run the script\n",
    "service = authenticate()\n",
    "download_all_attachments(service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV saved to: output/all_articles_combined.csv\n"
     ]
    }
   ],
   "source": [
    "# Folder setup\n",
    "input_dir = \"gmail_attachments\"  # Your folder with DOCX files\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "target_prefix = \"Results list for_50___\"\n",
    "\n",
    "# Split articles\n",
    "def split_articles_by_marker(paragraphs):\n",
    "    articles = []\n",
    "    current_article = []\n",
    "    for para in paragraphs:\n",
    "        current_article.append(para)\n",
    "        if \"End of Document\" in para:\n",
    "            articles.append(current_article)\n",
    "            current_article = []\n",
    "    if current_article:\n",
    "        articles.append(current_article)\n",
    "    return articles\n",
    "\n",
    "# extract fields from a single article block\n",
    "def extract_article_fields(article):\n",
    "    fields = {\n",
    "        \"headline\": \"\", \"source\": \"\", \"date\": \"\", \"copyright\": \"\",\n",
    "        \"section\": \"\", \"length\": \"\", \"byline\": \"\",\n",
    "        \"body\": \"\", \"notes\": \"\", \"load_date\": \"\"\n",
    "    }\n",
    "    body_lines, note_lines = [], []\n",
    "    in_body, in_notes = False, False\n",
    "    header_lines = []\n",
    "\n",
    "    for line in article:\n",
    "        if line == \"Body\":\n",
    "            in_body, in_notes = True, False\n",
    "            continue\n",
    "        elif line == \"Notes\":\n",
    "            in_body, in_notes = False, True\n",
    "            continue\n",
    "        elif \"End of Document\" in line:\n",
    "            in_body, in_notes = False, False\n",
    "            continue\n",
    "        elif line.startswith(\"Load-Date:\"):\n",
    "            fields[\"load_date\"] = line.replace(\"Load-Date:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Section:\"):\n",
    "            fields[\"section\"] = line.replace(\"Section:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Length:\"):\n",
    "            fields[\"length\"] = line.replace(\"Length:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Byline:\"):\n",
    "            fields[\"byline\"] = line.replace(\"Byline:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Copyright\"):\n",
    "            fields[\"copyright\"] += line + \" \"\n",
    "            continue\n",
    "\n",
    "        if in_body:\n",
    "            body_lines.append(line)\n",
    "        elif in_notes:\n",
    "            note_lines.append(line)\n",
    "        else:\n",
    "            header_lines.append(line)\n",
    "\n",
    "    if header_lines:\n",
    "        fields[\"headline\"] = header_lines[0]\n",
    "        source_lines = []\n",
    "        date_line = \"\"\n",
    "        for line in header_lines[1:]:\n",
    "            if re.search(r\"[A-Za-z]+\\s+\\d{1,2},\\s*\\d{4}\", line):\n",
    "                date_line = line\n",
    "                break\n",
    "            else:\n",
    "                source_lines.append(line)\n",
    "        fields[\"source\"] = \" \".join(source_lines)\n",
    "        fields[\"date\"] = date_line\n",
    "\n",
    "    fields[\"body\"] = \"\\n\".join(body_lines)\n",
    "    fields[\"notes\"] = \"\\n\".join(note_lines)\n",
    "    return fields\n",
    "\n",
    "# Process all DOCX files in the input directory\n",
    "all_data = []\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.startswith(target_prefix) and filename.lower().endswith(\".docx\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        doc = Document(file_path)\n",
    "        paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "        articles = split_articles_by_marker(paragraphs)\n",
    "        structured = [extract_article_fields(a) for a in articles]\n",
    "        for record in structured:\n",
    "            record[\"source_file\"] = filename\n",
    "        all_data.extend(structured)\n",
    "\n",
    "# Save all data to CSV\n",
    "df = pd.DataFrame(all_data)\n",
    "csv_path = os.path.join(output_dir, \"all_articles_combined.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"CSV saved to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optioanlly, do some basic data quality checks\n",
    "# Basic data quality examination\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Check for empty strings\n",
    "empty_strings = (df == '').sum()\n",
    "print(\"\\nEmpty strings per column:\")\n",
    "print(empty_strings)\n",
    "\n",
    "# Check date formats and extract year\n",
    "# Try to extract years from the date column\n",
    "years = []\n",
    "for date_str in df['date']:\n",
    "    match = re.search(r'\\b(19|20)\\d{2}\\b', date_str)\n",
    "    if match:\n",
    "        years.append(match.group(0))\n",
    "    else:\n",
    "        years.append(None)\n",
    "\n",
    "# Count of articles by year\n",
    "year_counts = Counter([y for y in years if y])\n",
    "print(\"\\nArticles by year:\")\n",
    "for year, count in sorted(year_counts.items()):\n",
    "    print(f\"{year}: {count}\")\n",
    "\n",
    "# Check length column - convert to numeric if possible\n",
    "def extract_word_count(length_str):\n",
    "    match = re.search(r'(\\d+)\\s*words', length_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "word_counts = [extract_word_count(length) for length in df['length']]\n",
    "word_counts = [wc for wc in word_counts if wc is not None]\n",
    "\n",
    "if word_counts:\n",
    "    print(f\"\\nWord count statistics:\")\n",
    "    print(f\"Min: {min(word_counts)}\")\n",
    "    print(f\"Max: {max(word_counts)}\")\n",
    "    print(f\"Average: {sum(word_counts)/len(word_counts):.1f}\")\n",
    "    print(f\"Articles with word count: {len(word_counts)}\")\n",
    "\n",
    "# Check source distribution\n",
    "top_sources = df['source'].value_counts().head(10)\n",
    "print(\"\\nTop 10 sources:\")\n",
    "print(top_sources)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
