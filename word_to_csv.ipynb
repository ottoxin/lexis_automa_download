{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Word document\n",
    "doc = Document(\"your_file.docx\")  # Replace with the actual file name\n",
    "doc.paragraphs\n",
    "\n",
    "# Extract all non-empty paragraphs\n",
    "paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "\n",
    "# Print basic information about the document\n",
    "print(f\"Document has {len(paragraphs)} paragraphs\")\n",
    "# Save the first 50 paragraphs to a text file\n",
    "with open(\"first_50_paragraphs.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\".join(paragraphs[:50]))\n",
    "print(\"First 50 paragraphs saved to 'first_50_paragraphs.txt'\")\n",
    "\n",
    "# Analyze content of articles\n",
    "# Ensure articles and df are defined\n",
    "if 'articles' in globals() and 'df' in globals():\n",
    "    articles_df = pd.DataFrame(articles)\n",
    "    if len(articles) > 0:\n",
    "        # Number of articles\n",
    "        print(f\"\\nTotal articles found: {len(articles)}\")\n",
    "        \n",
    "        # Years distribution (if 'year' column exists)\n",
    "        if 'year' in df.columns:\n",
    "            years = df['year'].value_counts().sort_index()\n",
    "            print(f\"\\nArticles by year:\\n{years}\")\n",
    "        else:\n",
    "            print(\"\\nNo 'year' column found in the DataFrame.\")\n",
    "        \n",
    "        # Common topics\n",
    "        from collections import Counter\n",
    "        import re\n",
    "        \n",
    "        # Get all words from headlines\n",
    "        all_words = ' '.join(df['headline'].str.lower()).split()\n",
    "        # Remove common stop words\n",
    "        stop_words = {'a', 'an', 'the', 'and', 'or', 'but', 'is', 'are', 'to', 'for', 'in', 'on', 'with', 'of', 'by', 'as'}\n",
    "        filtered_words = [word for word in all_words if word not in stop_words and len(word) > 2]\n",
    "        \n",
    "        # Count and display common words\n",
    "        word_counts = Counter(filtered_words)\n",
    "        most_common = word_counts.most_common(15)\n",
    "        print(\"\\nMost common words in headlines:\")\n",
    "        for word, count in most_common:\n",
    "            print(f\"  {word}: {count}\")\n",
    "        \n",
    "        # Length statistics (if 'body_length' and 'headline_length' columns exist)\n",
    "        if 'body_length' in df.columns and 'headline_length' in df.columns:\n",
    "            print(f\"\\nAverage body length: {df['body_length'].mean():.1f} characters\")\n",
    "            print(f\"Average headline length: {df['headline_length'].mean():.1f} characters\")\n",
    "        else:\n",
    "            print(\"\\nLength statistics columns not found in the DataFrame.\")\n",
    "        \n",
    "        # Article sources (if available)\n",
    "        if 'source' in df.columns and df['source'].notna().any():\n",
    "            sources = df['source'].value_counts().head(5)\n",
    "            print(f\"\\nTop sources:\\n{sources}\")\n",
    "else:\n",
    "    print(\"Variables 'articles' and 'df' are not defined in the current scope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Word document\n",
    "doc = Document(\"your_file.docx\")  # Replace with your actual file path\n",
    "\n",
    "# Extract all non-empty paragraphs\n",
    "paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "\n",
    "# Split articles using \"End of Document\" as a delimiter\n",
    "def split_articles_by_marker(paragraphs):\n",
    "    articles = []\n",
    "    current_article = []\n",
    "    for para in paragraphs:\n",
    "        current_article.append(para)\n",
    "        if \"End of Document\" in para:\n",
    "            articles.append(current_article)\n",
    "            current_article = []\n",
    "    if current_article:\n",
    "        articles.append(current_article)\n",
    "    return articles\n",
    "\n",
    "# Revised function to extract fields from a single article block\n",
    "def extract_article_fields(article):\n",
    "    fields = {\n",
    "        \"headline\": \"\",\n",
    "        \"source\": \"\",\n",
    "        \"date\": \"\",\n",
    "        \"copyright\": \"\",\n",
    "        \"section\": \"\",\n",
    "        \"length\": \"\",\n",
    "        \"byline\": \"\",\n",
    "        \"body\": \"\",\n",
    "        \"notes\": \"\",\n",
    "        \"load_date\": \"\"\n",
    "    }\n",
    "    \n",
    "    body_lines = []\n",
    "    note_lines = []\n",
    "    \n",
    "    # Flags for collecting body/notes\n",
    "    in_body = False\n",
    "    in_notes = False\n",
    "    \n",
    "    # Collect header lines (before any marker)\n",
    "    header_lines = []\n",
    "    \n",
    "    for line in article:\n",
    "        # Check if line is a marker\n",
    "        if line == \"Body\":\n",
    "            in_body = True\n",
    "            in_notes = False\n",
    "            continue\n",
    "        elif line == \"Notes\":\n",
    "            in_body = False\n",
    "            in_notes = True\n",
    "            continue\n",
    "        elif \"End of Document\" in line:\n",
    "            in_body = False\n",
    "            in_notes = False\n",
    "            continue\n",
    "        elif line.startswith(\"Load-Date:\"):\n",
    "            fields[\"load_date\"] = line.replace(\"Load-Date:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Section:\"):\n",
    "            fields[\"section\"] = line.replace(\"Section:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Length:\"):\n",
    "            fields[\"length\"] = line.replace(\"Length:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Byline:\"):\n",
    "            fields[\"byline\"] = line.replace(\"Byline:\", \"\").strip()\n",
    "            continue\n",
    "        elif line.startswith(\"Copyright\"):\n",
    "            fields[\"copyright\"] += line + \" \"\n",
    "            continue\n",
    "        \n",
    "        # Depending on our state, add line to header, body, or notes\n",
    "        if in_body:\n",
    "            body_lines.append(line)\n",
    "        elif in_notes:\n",
    "            note_lines.append(line)\n",
    "        else:\n",
    "            header_lines.append(line)\n",
    "    \n",
    "    # Process header_lines:\n",
    "    # Assume the first line is the headline\n",
    "    if header_lines:\n",
    "        fields[\"headline\"] = header_lines[0]\n",
    "        \n",
    "        # Next, concatenate subsequent lines (until we hit a date-like pattern) as source.\n",
    "        source_lines = []\n",
    "        date_line = \"\"\n",
    "        for line in header_lines[1:]:\n",
    "            # Check for a date pattern (e.g., \"November 18, 2024\")\n",
    "            if re.search(r\"[A-Za-z]+\\s+\\d{1,2},\\s*\\d{4}\", line):\n",
    "                date_line = line\n",
    "                break\n",
    "            else:\n",
    "                source_lines.append(line)\n",
    "        fields[\"source\"] = \" \".join(source_lines)\n",
    "        fields[\"date\"] = date_line\n",
    "\n",
    "    fields[\"body\"] = \"\\n\".join(body_lines)\n",
    "    fields[\"notes\"] = \"\\n\".join(note_lines)\n",
    "    \n",
    "    return fields\n",
    "\n",
    "# Process the document and extract articles\n",
    "articles = split_articles_by_marker(paragraphs)\n",
    "structured_data = [extract_article_fields(article) for article in articles]\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df = pd.DataFrame(structured_data)\n",
    "csv_path = \"output_articles_revised.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV saved to {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
